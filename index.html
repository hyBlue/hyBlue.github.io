<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Hyunyoung Jung</title>

  <meta name="author" content="Hyunyoung Jung">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon"
    href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Hyunyoung Jung</name>
                  </p>
                  <p>
                    I am a Computer Vision Engineer on the Core AI team at Meta Reality Labs, working on 3D vision and generative models for mixed reality applications.
                    I obtained my Ph.D. in Computer Science and Engineering (CSE) from Seoul National University under the guidance of Prof. <a
                      href="http://cmalab.snu.ac.kr/member/yeonbin">Sungjoo Yoo</a>.
                    During my doctoral studies, I completed research internships at Meta in 2022 and 2023. I received a B.S. in CSE from Seoul National University.
                  </p>
                  <p style="text-align:center">
                    <a href="https://drive.google.com/file/d/1MYc9EuSvopnszcYnqjWMVatY3oLOa06a/view?usp=sharing">CV</a> &nbsp/&nbsp
                    <a href="mailto:gusdud1500@gmail.com">Email</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=PvpSNkQAAAAJ&hl=ko">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://www.linkedin.com/in/hyunyoungjung/">Linkedin</a> <!--&nbsp/&nbsp-->
                    <!-- <a href="https://hyblue.github.io/blog">Blog</a> -->

                  </p>
                </td>
                <td style="padding:2.5%;width:30%;max-width:30%">
                  <img style="width:100%;max-width:100%" alt="profile photo" src="images/hyunyoung.jpg"
                    class="hoverZoomLink"></a>
                </td>
              </tr>

            </tbody>
          </table>


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Research</heading>

                </td>
              </tr>

            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="geosrf_stop()" onmouseover="geosrf_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='geosrf_img'><video width=100% muted autoplay loop>
                        <source src="images/geosrf.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/geosrf.jpg' width="100%">
                  </div>
                  <script type="text/javascript">
                    function geosrf_start() {
                      document.getElementById('geosrf_img').style.opacity = "1";
                    }

                    function geosrf_stop() {
                      document.getElementById('geosrf_img').style.opacity = "0";
                    }
                    geosrf_stop()
                  </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://www.youtube.com/watch?v=QpFQjE9sDg8">
                    <papertitle>Geometry Transfer for Stylizing Radiance Fields </papertitle>
                  </a>
                  <br>
                  <strong>Hyunyoung Jung</strong>, <a href="https://shnnam.github.io/">Seonghyeon
                    Nam</a>, <a href="https://nsarafianos.github.io/">Nikolaos Sarafianos</a>, <a
                    href="http://cmalab.snu.ac.kr/">Sungjoo Yoo</a>, <a href="https://www.sornlex.com/">Alexander
                    Sorkine-Hornung</a>, <a href="https://www.linkedin.com/in/rakesh-r-3848538">Rakesh Ranjan</a>

                  <br>
                  <em>CVPR</em>, 2024
                  <br>
                  <a
                    href="https://drive.google.com/file/d/18REIVi7IQCLhgZ_nhZKdVl6jQ-6bj3h5/view?usp=drive_link">paper</a>
                  /
                  <a href="https://hyblue.github.io/geo-srf">project page</a> /
                  <a href="https://arxiv.org/abs/2402.00863">arXiv</a>

                  <br><br>
                  The geometry and appearance of a 3D scene are coherently stylized, guided by an RGB image and a depth
                  map.


                </td>
              </tr>


              <tr onmouseout="anyflow_stop()" onmouseover="anyflow_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='anyflow_img'>
                      <img src='images/anyflow_after.png' width="160">
                    </div>
                    <img src='images/anyflow_before.png' width="160">
                  </div>
                  <script type="text/javascript">
                    function anyflow_start() {
                      document.getElementById('anyflow_img').style.opacity = "1";
                    }

                    function anyflow_stop() {
                      document.getElementById('anyflow_img').style.opacity = "0";
                    }
                    anyflow_stop()
                  </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a
                    href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jung_AnyFlow_Arbitrary_Scale_Optical_Flow_With_Implicit_Neural_Representation_CVPR_2023_paper.pdf">
                    <papertitle>AnyFlow: Arbitrary Scale Optical Flow with Implicit Neural Representation</papertitle>
                  </a>
                  <br>
                  <strong>Hyunyoung Jung</strong>, <a href="https://huizhuo1987.github.io/"> Zhuo Hui</a>,
                  <a href="https://www.linkedin.com/in/rayluolei/">Lei Luo</a>,
                  <a href="https://www.linkedin.com/in/haitao-yang-28582324/">Haitao Yang</a> ,
                  <a href="https://pages.cs.wisc.edu/~fliu/">Feng Liu</a> ,
                  <a href="http://cmalab.snu.ac.kr/">Sungjoo Yoo</a>,
                  <a href="https://www.linkedin.com/in/rakesh-r-3848538">Rakesh Ranjan</a>,
                  <a href="https://www.linkedin.com/in/demandolx/">Denis Demandolx</a>


                  <br>
                  <em>CVPR</em>, 2023 &nbsp <font color="red"><strong>(Highlight)</strong></font>
                  <br>
                  <a
                    href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jung_AnyFlow_Arbitrary_Scale_Optical_Flow_With_Implicit_Neural_Representation_CVPR_2023_paper.pdf">paper</a>/
                  <a href="http://arxiv.org/abs/2303.16493">arXiv</a>

                  <br><br>
                  The optical flow network is designed to produce outputs at any desired resolution while maintaining
                  robust performance, when processing low-resolution images.

                </td>
              </tr>

              <tr onmouseout="fsre_stop()" onmouseover="fsre_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='fsre_image'>
                      <img src='images/fsre_after.png' width="160">
                    </div>
                    <img src='images/fsre_before.png' width="160">
                  </div>
                  <script type="text/javascript">
                    function fsre_start() {
                      document.getElementById('fsre_image').style.opacity = "1";
                    }

                    function fsre_stop() {
                      document.getElementById('fsre_image').style.opacity = "0";
                    }
                    fsre_stop()
                  </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a
                    href="https://openaccess.thecvf.com/content/ICCV2021/papers/Jung_Fine-Grained_Semantics-Aware_Representation_Enhancement_for_Self-Supervised_Monocular_Depth_Estimation_ICCV_2021_paper.pdf">
                    <papertitle>Fine-grained Semantics-aware Representation Enhancement for Self-supervised Monocular
                      Depth Estimation</papertitle>
                  </a>
                  <br>
                  <strong>Hyunyoung Jung</strong>,
                  <a href="https://sites.google.com/view/eh-p/members">Eunhyeok Park</a>,
                  <a href="http://cmalab.snu.ac.kr/">Sungjoo Yoo</a>

                  <br>
                  <em>ICCV</em>, 2021 &nbsp <font color="red"><strong>(Oral)</strong></font>
                  <br>
                  <a
                    href="https://openaccess.thecvf.com/content/ICCV2021/papers/Jung_Fine-Grained_Semantics-Aware_Representation_Enhancement_for_Self-Supervised_Monocular_Depth_Estimation_ICCV_2021_paper.pdf">paper</a>
                  /
                  <a href="https://arxiv.org/abs/2108.08829">arXiv</a>
                  /
                  <a href="https://github.com/hyBlue/FSRE-Depth">code</a>

                  <br><br>
                  The depth estimation network utilizes semantic information to enhance boundary accuracy, incorporating
                  metric-learning and cross-attention.
                </td>
              </tr>

            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Projects</heading>

                </td>
              </tr>

            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="ragraf_stop()" onmouseover="ragraf_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='ragraf_img'>
                      <img src='images/ragraf_after.gif' width="160">
                    </div>
                    <img src='images/ragraf_before.png' width="160">
                  </div>
                  <script type="text/javascript">
                    function ragraf_start() {
                      document.getElementById('ragraf_img').style.opacity = "1";
                    }

                    function ragraf_stop() {
                      document.getElementById('ragraf_img').style.opacity = "0";
                    }
                    ragraf_stop()
                  </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">

                  <papertitle>Reflectance-aware Generative Radiance Fields for 3D-aware Image Synthesis</papertitle>
                  <br>2021

                  <br>
                  <br>
                  The generative NeRF-based network is trained to achieve relightability using only a collection of
                  single-view images, without requiring any supplementary information.
                </td>
              </tr>


              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle;text-align: center"><img
                    style="width:100%;max-width:75%" , src="images/iitp.png"></td>

                <td style="padding:20px;width:75%;vertical-align:middle">

                  <papertitle>Development of model compression framework for scalable on-device AI computing on Edge
                    applications</papertitle>

                  <br><br>
                  A Korean government funded project ($10M, 2021-2024) to develop automatic DL model optimization
                  methods for on-device AI on commercial neural network accelerators.
                </td>
              </tr>





            </tbody>
          </table>





          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <heading>Experience</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table width="100%" align="center" border="0" cellpadding="20">
            <tbody>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle;text-align: center"><img
                    style="width:100%;max-width:75%" , src="images/meta.gif"></td>
                <td width="75%" valign="center">
                  Research Intern at <a href="https://about.meta.com/realitylabs/">Meta Reality Labs</a>
                  <br><br>
                  <em> Jun 2023 - Dec 2023, &nbsp Sunnyvale, CA, US </em>

                  <br>
                  <em> Jun 2022 - Dec 2022, &nbsp Seattle, WA, US </em>

                </td>
              </tr>


              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle;text-align: center"><img
                    style="width:100%;max-width:75%" , src="images/bobidi.jpg"></td>
                <td width="75%" valign="center">
                  Software Engineer Intern at <a href="https://www.bobidi.com/">Bobidi</a>
                  <br><br>
                  <em> Jan 2022 - Feb 2022, &nbsp Seoul, South Korea </em>

                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle;text-align: center"><img
                    style="width:50%;max-width:50%" , src="images/line.png"></td>
                <td width="75%" valign="center">
                  Software Engineer Intern at <a href="https://linepluscorp.com/">Line Plus Corp.</a>
                  <br><br>
                  <em> Jan 2018 - Feb 2018, &nbsp Seongnam, South Korea </em>

                </td>
              </tr>
            </tbody>
          </table>


          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <heading>Honors & awards</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table width="100%" align="center" border="0" cellpadding="20">
            <tbody>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle;text-align: center"><img
                    style="width:100%;max-width:75%" , src="images/qualcomm.png"></td>
                <td width="75%" valign="center">
                  <a
                    href="https://www.qualcomm.com/research/research/university-relations/innovation-fellowship/winners">
                    Qualcomm Innovation Fellowship Korea 2021
                  </a>
                  <br>
                  <em> Nov 2021 </em>

                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle;text-align: center"><img
                    style="width:100%;max-width:75%" , src="images/naver_labs.jpg"></td>
                <td width="75%" valign="center">
                  <a href="https://www.naverlabs.com/storyDetail/181">
                    NAVER LABS Mapping & Localization Challenge
                  </a>
                  <br>
                  <em> Jul 2020 </em>
                  <br><br>
                  Built full pipeline of structure-based hierarchical visual localization framework on NAVER LABS
                  datasets. Earned 2nd Place in both Indoor / Outdoor
                  sections (total prize 6M KRW).
                </td>
              </tr>

            </tbody>
          </table>


          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <heading>Education</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table width="100%" align="center" border="0" cellpadding="20">
            <tbody>

              <tr>
                <td width="75%" valign="center">
                  Seoul National University
                  <br>
                  <br>
                  <em> Sep 2019 - Present </em>
                  <br>
                  The Integrated MA/Ph.D. Course in Computer Science and Engineering
                  <br>
                  <br>
                  <em> Mar 2013 - Aug 2019 (two gap years for military service)</em>
                  <br>
                  Bachelor of Science in Computer Science and Engineering
                  <br>

                </td>
              </tr>


            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    Forked from <a href="https://jonbarron.info/">Jon Barron's website </a>.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>
